# -*- coding: utf-8 -*-
"""06_LSTM_Ramkumar_Sivakumar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xoREJBID3pzwsepsNMD0ZmAIkakhydR5

## Import Libraries and Preprocess Data
"""

import pandas as pd
import os
import json

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Complaints/processed_complaints.csv")

data = data.drop(['Unnamed: 0'],1)
data.head(5)

data['Product'].value_counts()

product_map = []
product_dict = {}
for i,prod in enumerate(data['Product'].unique()):
    product_map.append({"Product":prod, "id":i})
    product_dict[prod] = i

product_dict

"""## PART 1: Predicting Subject"""

data['output'] = data['Product'].map(product_dict)

data.head()

data = data[['Complaint','output']]

data.head()

data['Complaint'] = data['Complaint'].astype(str)

!pip install nltk

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
import nltk
nltk.download("stopwords")
stop_words = stopwords.words('english')
STOPWORDS = set(stopwords.words('english'))
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

MAX_NB_WORDS = 5000
MAX_SEQUENCE_LENGTH = 50
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(data['Complaint'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens' % len(word_index))

X = tokenizer.texts_to_sequences(data['Complaint'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

Y = pd.get_dummies(data['output']).values
print('Shape of label tensor:', Y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

X.shape

Y.shape



"""### Build Model"""

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.compat.v1.keras.layers import CuDNNLSTM
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
#model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(CuDNNLSTM(100))
model.add(Dense(7, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

Y[1]

epochs = 10
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)

epochs = 10
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss')])

epochs = 1
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss')])

epochs = 1
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss')])

import pickle

#model.save("/content/drive/MyDrive/Complaints/Model")
#pickle.dump(model, open("/content/drive/MyDrive/Complaints/Model", 'wb'))
model.save_weights('/content/drive/MyDrive/Complaints/Model.h5')

import matplotlib.pyplot as plt

plt.title(label='Loss')
plt.plot(history.history['loss'], label = "Train Loss")
plt.plot(history.history['val_loss'], label = "Validation Loss")
plt.legend()

plt.title(label='Accuracy')
plt.plot(history.history['accuracy'], label = "Train Accuracy")
plt.plot(history.history['val_accuracy'], label = "Validation Accuracy")
plt.legend()

pred = model.predict(X_test)

from sklearn.metrics import classification_report

print(classification_report(np.argmax(pred, axis=1), np.argmax(Y_test, axis = 1)))
