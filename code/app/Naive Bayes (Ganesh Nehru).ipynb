{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151442d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset. \n",
    "df = pd.read_csv('complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27603f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame before performing data cleanup.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa151d80",
   "metadata": {},
   "source": [
    "##### Since we are only focused on the products and complaint narratives, we will re-create the data frame using only the 'Product' and 'Consumer complaint narrative' attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02745a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Product' , 'Consumer complaint narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09555f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename attributes of dataframe for simplicity.\n",
    "df = df.rename(columns={'Product' : \"Product\",\n",
    "                       'Consumer complaint narrative' : \"Complaint\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame after renaming attributes.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbedf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Product').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a89fa7",
   "metadata": {},
   "source": [
    "##### Many of the products of the same category can be merged together into a single product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up data by renaming similar products into one category of products.\n",
    "df['Product'].replace({'Bank account or service': 'Banking Services',\n",
    "                       'Checking or savings account' : 'Banking Services',\n",
    "                       'Consumer Loan' : 'Loans',\n",
    "                       'Credit card' : 'Credit/Prepaid Cards',\n",
    "                       'Credit card or prepaid card' : 'Credit/Prepaid Cards',\n",
    "                       'Credit reporting' : 'Credit Reporting and Services',\n",
    "                       'Credit reporting, credit repair services, or other personal consumer reports' : 'Credit Reporting and Services',\n",
    "                       'Debt collection' : 'Debt Collection',\n",
    "                       'Money transfer, virtual currency, or money service' : 'Banking Services',\n",
    "                       'Money transfers' : 'Banking Services',\n",
    "                       'Mortgage' : 'Mortgages',\n",
    "                       'Other financial service' : 'Banking Services',\n",
    "                       'Payday loan' : 'Loans',\n",
    "                       'Payday loan, title loan, or personal loan' : 'Loans',\n",
    "                       'Prepaid card' : 'Credit/Prepaid Cards',\n",
    "                       'Student loan' : 'Loans',\n",
    "                       'Vehicle loan or lease' : 'Loans',\n",
    "                       'Virtual currency' : 'Crypto Currency'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a80314",
   "metadata": {},
   "source": [
    "#Refined products and their counts.\n",
    "df.groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07da586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame with refined product categories.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fd63",
   "metadata": {},
   "source": [
    "### Cleanup data frame of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new df to hold only non-null values of Consumer complaint narratives.\n",
    "df = df[pd.notnull(df['Complaint'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d866e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "stop_words += [\"''\", '\"\"', '...', '``', '--', 'XXXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize complaint data and remove stop words from complaint narrative.\n",
    "def processComplaint(comp):\n",
    "    tokens = nltk.word_tokenize(comp)\n",
    "    removed_stop_words = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    new_removed_stop_words = [word for word in removed_stop_words if word.isalpha()]\n",
    "    \n",
    "    return new_removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7772aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link words together.\n",
    "def linkWords(words):\n",
    "    linked_words = ''\n",
    "    \n",
    "    for w in words:\n",
    "        linked_words += w + ' '\n",
    "    \n",
    "    return linked_words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e12440",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group variants of the same word and merge complaints.\n",
    "def lemLink(words):\n",
    "    words = [word for word in words if word is not np.nan]\n",
    "    \n",
    "    lem_list = []\n",
    "    \n",
    "    for idx, word in enumerate(words):\n",
    "        lem_list.append(lm.lemmatize(word))\n",
    "    \n",
    "    linked_str = linkWords(lem_list)\n",
    "    \n",
    "    return linked_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    processed_complaints = processComplaint(df['Complaint'].iloc[i])\n",
    "    complaint = lemLink(processed_complaints)\n",
    "    df['Complaint'].iloc[i] = complaint\n",
    "    \n",
    "    if i % 5000 == 0:\n",
    "        print(f'Processed row #: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bad24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
