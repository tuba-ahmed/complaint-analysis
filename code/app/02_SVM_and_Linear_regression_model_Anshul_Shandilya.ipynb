{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Our Project**\n",
    "\n",
    "For our project, we decided to present 4 models our our text classification problem. The end goal for our project is to compare all four models and pick which one performed the best. \n",
    "\n",
    "For this project, I picked SVM to perform on the dataset. \n",
    "\n",
    "Scoring metric to be used is F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Goals for this notebook:**\n",
    "\n",
    "* Perform cleaning and prepping of data\n",
    "* Choose a metric for scoring\n",
    "* Go through SVM model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the processed dataset created in the '02_Data_Exploration_Anshul_Shandilya' notebook.\n",
    "df = pd.read_csv('data/cleaned_complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1112420, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>sub_product</th>\n",
       "      <th>issue</th>\n",
       "      <th>sub_issue</th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>debt_collection</td>\n",
       "      <td>Other debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>XXXX. I do not owe any money to XXXX  XXXX. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debt_collection</td>\n",
       "      <td>Other debt</td>\n",
       "      <td>False statements or representation</td>\n",
       "      <td>Indicated you were committing crime by not pay...</td>\n",
       "      <td>XXXX is attempting to collect funds for Valuat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit_reporting_and_services</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Credit inquiries on your report that you don't...</td>\n",
       "      <td>EXPERIAN I didnt consent to these Inquiries Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banking_services_and_transfers</td>\n",
       "      <td>Savings account</td>\n",
       "      <td>Managing an account</td>\n",
       "      <td>Deposits and withdrawals</td>\n",
       "      <td>Citibank froze my account that contained {$200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>credit_reporting_and_services</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Reporting company used your report improperly</td>\n",
       "      <td>In accordance with the fair credit reporting a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          product       sub_product  \\\n",
       "0                 debt_collection        Other debt   \n",
       "1                 debt_collection        Other debt   \n",
       "2   credit_reporting_and_services  Credit reporting   \n",
       "3  banking_services_and_transfers   Savings account   \n",
       "4   credit_reporting_and_services  Credit reporting   \n",
       "\n",
       "                                issue  \\\n",
       "0   Attempts to collect debt not owed   \n",
       "1  False statements or representation   \n",
       "2         Improper use of your report   \n",
       "3                 Managing an account   \n",
       "4         Improper use of your report   \n",
       "\n",
       "                                           sub_issue  \\\n",
       "0                                      Debt was paid   \n",
       "1  Indicated you were committing crime by not pay...   \n",
       "2  Credit inquiries on your report that you don't...   \n",
       "3                           Deposits and withdrawals   \n",
       "4      Reporting company used your report improperly   \n",
       "\n",
       "                                           narrative  \n",
       "0  XXXX. I do not owe any money to XXXX  XXXX. I ...  \n",
       "1  XXXX is attempting to collect funds for Valuat...  \n",
       "2  EXPERIAN I didnt consent to these Inquiries Al...  \n",
       "3  Citibank froze my account that contained {$200...  \n",
       "4  In accordance with the fair credit reporting a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the 3rd time I try to fix this issue with Transunion. I have contact and retained XXXX XXXX XXXX, Your Credit Reporting Agency ( Transunion ) has continued to allow these accounts to report in my name materially false and fraudulent accounts. \\n\\nWe will also be sending out registered mail to the agency and dependent on the outcome we will choose the next legal steps. \\n\\nPlease cease and desist all reporting of these accounts : XXXX XXXX XXXX - {$320.00} XXXX XXXX XXXX - XXXX XXXX XXXX XXXX - XXXX Failure to fulfill your fiduciary duty as a data furnisher will inevitably result in a lawsuit filed by XXXX XXXX XXXX on my behalf. You have a fiduciary duty under the FCRA 605 ( B ) to respond within 4 business days of receipt. \\n\\nFCRA 605 ( b ) reads ; A consumer reporting agency shall block the reporting of any information in the file of a consumer, not later then 4 business days after the date of receipt.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[345]['narrative']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we still have the narrative text in normal text. Let's see if we can do something about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akaid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akaid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akaid\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += [\"XXXX\", \"xxxx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text(text):\n",
    "  '''\n",
    "    Function to process the text and return a list of words with stopwords and punctuations removed\n",
    "  '''\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Revove tokens with stop words removed.\n",
    "  ret_tokens_st = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "\n",
    "  # Remove tokens with 2 or more consecutive x's\n",
    "  for token in ret_tokens_st:\n",
    "    if re.search(r'x{2,}', token):\n",
    "        ret_tokens_st.remove(token)\n",
    "\n",
    "  # Remove tokens with string that contains two or more consecutive X's\n",
    "  for token in ret_tokens_st:\n",
    "    if re.search(r'X{2,}', token):\n",
    "        ret_tokens_st.remove(token)\n",
    "\n",
    "\n",
    "  # Remove tokens with 2 or more consecutive -'s\n",
    "  for token in ret_tokens_st:\n",
    "    if re.search(r'-{2,}', token):\n",
    "        ret_tokens_st.remove(token)\n",
    "\n",
    "  # Remove tokens with 2 or more consecutive .'s\n",
    "  for token in ret_tokens_st:\n",
    "    if re.search(r'\\.{2,}', token):\n",
    "        ret_tokens_st.remove(token)\n",
    "\n",
    "  # Remove tokens with float numbers\n",
    "  for token in ret_tokens_st:\n",
    "    if re.search(r'\\d+\\.\\d+', token):\n",
    "        ret_tokens_st.remove(token)\n",
    "\n",
    "  # Remove tokens with date in format xx/xx/xxxx\n",
    "  for token in ret_tokens_st:\n",
    "    if re.search(r'\\d+/\\d+/\\d+', token):\n",
    "        ret_tokens_st.remove(token)\n",
    "      \n",
    "        \n",
    "  # Remove tokens with numbers  \n",
    "  ret_tokens = [token for token in ret_tokens_st if not token.isnumeric()]\n",
    "\n",
    "  return ret_tokens\n",
    "\n",
    "# function to concat words (used in function below)\n",
    "def concat_words(list_of_words):\n",
    "    # remove any NaN's\n",
    "    # list_of_words = [i for i in list if i is not np.nan]\n",
    "\n",
    "    concat_words = ''\n",
    "    for word in list_of_words:\n",
    "        concat_words += word + ' '\n",
    "    return concat_words.strip()\n",
    "\n",
    "def perform_lemmatization(text):\n",
    "  '''\n",
    "      Function to perform lemmatization on the text and return concatenated string of lemmatized words separated by space.\n",
    "  '''\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "  return ' '.join(lemmatized_words)\n",
    "\n",
    "  # # lemmatize each word\n",
    "  # lemmatizer = WordNetLemmatizer()\n",
    "  # lemmatized_list = []\n",
    "  # for idx, word in enumerate(text):\n",
    "  #     lemmatized_list.append(lemmatizer.lemmatize(word))\n",
    "  \n",
    "  # # make the list into a single string with the words separated by ' '\n",
    "  # lemmatized_text = concat_words(lemmatized_list)\n",
    "\n",
    "\n",
    "  # return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556210.0\n"
     ]
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since working on this dataset, the dataset is huge. So we will work only on randomly selected sample of 10000 from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "sample_df = df.sample(sample_size, random_state=42)\n",
    "sample_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6)\n"
     ]
    }
   ],
   "source": [
    "sample_df.head()\n",
    "print(sample_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akaid\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>product</th>\n",
       "      <th>sub_product</th>\n",
       "      <th>issue</th>\n",
       "      <th>sub_issue</th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67599</td>\n",
       "      <td>loans</td>\n",
       "      <td>Loan</td>\n",
       "      <td>Getting a loan or lease</td>\n",
       "      <td>Fraudulent loan</td>\n",
       "      <td>affiant transaction financed westlake assigned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>281147</td>\n",
       "      <td>loans</td>\n",
       "      <td>Loan</td>\n",
       "      <td>Problems at the end of the loan or lease</td>\n",
       "      <td>Unable to receive car title or other problem a...</td>\n",
       "      <td>paid vehicle 'm finding company never released...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>765663</td>\n",
       "      <td>credit_reporting_and_services</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a credit reporting company's inve...</td>\n",
       "      <td>Their investigation did not fix an error on yo...</td>\n",
       "      <td>filing several dispute equifax still refuse re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1032578</td>\n",
       "      <td>credit_reporting_and_services</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a credit reporting company's inve...</td>\n",
       "      <td>Their investigation did not fix an error on yo...</td>\n",
       "      <td>experian refuse proper investigation remove er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95117</td>\n",
       "      <td>credit_reporting_and_services</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Reporting company used your report improperly</td>\n",
       "      <td>accordance fair credit reporting act account v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                        product       sub_product  \\\n",
       "0    67599                          loans              Loan   \n",
       "1   281147                          loans              Loan   \n",
       "2   765663  credit_reporting_and_services  Credit reporting   \n",
       "3  1032578  credit_reporting_and_services  Credit reporting   \n",
       "4    95117  credit_reporting_and_services  Credit reporting   \n",
       "\n",
       "                                               issue  \\\n",
       "0                            Getting a loan or lease   \n",
       "1           Problems at the end of the loan or lease   \n",
       "2  Problem with a credit reporting company's inve...   \n",
       "3  Problem with a credit reporting company's inve...   \n",
       "4                        Improper use of your report   \n",
       "\n",
       "                                           sub_issue  \\\n",
       "0                                    Fraudulent loan   \n",
       "1  Unable to receive car title or other problem a...   \n",
       "2  Their investigation did not fix an error on yo...   \n",
       "3  Their investigation did not fix an error on yo...   \n",
       "4      Reporting company used your report improperly   \n",
       "\n",
       "                                           narrative  \n",
       "0  affiant transaction financed westlake assigned...  \n",
       "1  paid vehicle 'm finding company never released...  \n",
       "2  filing several dispute equifax still refuse re...  \n",
       "3  experian refuse proper investigation remove er...  \n",
       "4  accordance fair credit reporting act account v...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(sample_size):\n",
    "    processed = process_text(sample_df['narrative'].loc[i])\n",
    "    processed_lemm = perform_lemmatization(processed)\n",
    "    sample_df['narrative'].loc[i] = processed_lemm\n",
    "\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the 10000 sample dataset.\n",
    "\n",
    "Now that we have the processed complaints with processed narrative, we will save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('data/processed_sample_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have loaded the dataset. Probably 'sub_product', 'Issue' and 'sub_issue' rows are redundant for our initial work as I only plan to use the 'product' as the class labels and the 'narrative' as the training data. \n",
    "\n",
    "##### Initial Steps using the SVC algorithm:\n",
    "\n",
    "* Extract the labels using preprocessing.LabelEncoder()\n",
    "* Split the data using the train_test_split() function into 20% test size and the rest as training data. Shuffle will we set as True to randomise the data\n",
    "* Use the 'narrative' row as our training data\n",
    "* Build the initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = preprocessing.LabelEncoder()\n",
    "labels = encoded_labels.fit_transform(sample_df['product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into test and train data (20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(sample_df['narrative'], labels, stratify = labels, test_size=0.2, random_state=47, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000,)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we have our train and test data, we will build a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will initialise a TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will apply the TF-IDF model on both the test and train data (will take time since the num. of data is high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=3, ngram_range=(1, 3), smooth_idf=1,\n",
       "                stop_words='english', strip_accents='unicode', sublinear_tf=1,\n",
       "                token_pattern='\\\\w{1,}', use_idf=1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(list(x_train) + list(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf =  tfidf.transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training the SVM model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learned that SVM model takes a lot of time to run. Especially with close to 90000 components, it probably won't end before at least a couple of hours. So need to reduce the number of components before I proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the number of components. (Using Singular Value Decomposition)\n",
    "\n",
    "Initially, will try reducing to 300 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and applying SVD on the TF-IDF vectorized data to reduce the num. of components to 250.\n",
    "svd = TruncatedSVD(n_components=250)\n",
    "svd.fit(x_train_tfidf)\n",
    "x_train_svd = svd.transform(x_train_tfidf)\n",
    "x_test_svd = svd.transform(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the components reduced (using SVD), since SMV is a linear model, we need to normalize the data before we try to fit the data into SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data using StandardScaler.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(x_train_svd)\n",
    "x_train_svd_scl = scl.transform(x_train_svd)\n",
    "x_test_svd_scl = scl.transform(x_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and traininng a SVC model\n",
    "svm_model = SVC(C=1.0, probability=True)\n",
    "svm_model.fit(x_train_svd_scl, y_train)\n",
    "preds = svm_model.predict(x_test_svd_scl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8550971142924789\n"
     ]
    }
   ],
   "source": [
    "# Printing the F-1 score\n",
    "print(\"F1 score: \", metrics.f1_score(y_test, preds, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I will try **logistic regression** below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the alternatives for TF-IDF is count vectorizer feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and training a count vector model\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3), stop_words = 'english')\n",
    "count_vect.fit(list(x_train) + list(x_test))\n",
    "xtrain_count =  count_vect.transform(x_train)\n",
    "xtest_count = count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save xtrain_count and xtest_count to disk for future use.\n",
    "np.save('data/xtrain_count.npy', xtrain_count)\n",
    "np.save('data/xtest_count.npy', xtest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akaid\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Initializing and training a logistic regression model\n",
    "logistic_model = LogisticRegression(C=1.0)\n",
    "logistic_model.fit(xtrain_count, y_train)\n",
    "preds = logistic_model.predict(xtest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.854402883400011\n"
     ]
    }
   ],
   "source": [
    "# Printing the F1 score\n",
    "print(\"F1 score: \", metrics.f1_score(y_test, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Observations:**\n",
    "\n",
    "* For sample_size=10000, two  models were trained:\n",
    "\n",
    "    * **SVM** - F1 score = 0.8347134811772894\n",
    "    \n",
    "    * **Logistic Regression** - F1 score = 0.8049101124053899\n",
    "\n",
    "    **For 10,000 samples taken from the dataset, SVM performed relatively better than Logistic Regression.** \n",
    "\n",
    "<br>\n",
    "\n",
    "Since I took 10,000 samples, which is a fraction of close to million columns of data, I re-tried the same process with 100,000 samples.\n",
    "\n",
    "* For sample_size=100,000, two  models were trained:\n",
    "\n",
    "    * **SVM** - F1 score = 0.8550971142924789\n",
    "    \n",
    "    * **Logistic Regression** - F1 score = 0.854402883400011\n",
    "\n",
    "    **But for 100,000 samples taken from the dataset, SVM still performed better than Logistic Regression but the differnece between them was relatively smaller.**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9af26967f52218773d7bb0f1bf34c2bf9b6be081403aa8b1e9693beec522ca38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
