# -*- coding: utf-8 -*-
"""LDA-257-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WLFg1nRKTipKR2TaGZ8hNARu_X-yRtr
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df =pd.read_csv("drive/MyDrive/CMPE-257-Project/complaints.csv")
df.head()

pip install pyLDAvis

import numpy as np

import re
import string

import spacy

import gensim
from gensim import corpora

import pyLDAvis
import matplotlib.pyplot as plt
import seaborn as sns

def text_clean(text): 
    delete_dict = {sp_character: '' for sp_character in string.punctuation} 
    delete_dict[' '] = ' ' 
    table = str.maketrans(delete_dict)
    text1 = text.translate(table)
    #print('cleaned:'+text1)
    textArr= text1.split()
    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) 
    
    return text2.lower()

import nltk
nltk.download('stopwords')

df = df.rename(columns={'Product' : "Product",
                       'Consumer complaint narrative' : "Complaint"})

df = df[pd.notnull(df['Complaint'])]
df.head()

sample_df = df.sample(100000, random_state=42)
complaint_text = sample_df['Complaint']

complaint_text = complaint_text.apply(text_clean)

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
# function to remove stopwords
def remove_stopwords(text):
    textArr = text.split(' ')
    rem_text = " ".join([i for i in textArr if i not in stop_words])
    return rem_text

complaint_text = complaint_text.apply(remove_stopwords)

pip install spacy

!python -m spacy download en_core_web_md

nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): 
       output = []
       for sent in texts:
             doc = nlp(sent) 
             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])
       return output

text_list = complaint_text.tolist()
tokenized_complaints = lemmatization(text_list)
print(tokenized_complaints[1])

dictionary = corpora.Dictionary(tokenized_complaints)
doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_complaints]

# Creating the object for LDA model using gensim library
LDA = gensim.models.ldamodel.LdaModel

# Build LDA model
lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=10, random_state=100,
                chunksize=1000, passes=50,iterations=100)

lda_model.print_topics()

import pyLDAvis.gensim_models

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)
vis